@article{Samadi2020Decentralized,
	author = {Samadi, Esmat and Badri, Ali and Ebrahimpour, Reza},
	journal = {International Journal of Electrical Power &amp; Energy Systems},
	year = {2020},
	month = {11},
	pages = {106211},
	publisher = {Elsevier BV},
	title = {Decentralized multi-agent based energy management of microgrid using reinforcement learning},
	volume = {122},
	url = {http://dx.doi.org/10.1016/j.ijepes.2020.106211},
	doi = {10.1016/j.ijepes.2020.106211},
	abstract = {Abstract This paper proposes a multi-agent based decentralized energy management approach in a grid-connected microgrid (MG). The MG comprises of wind and photovoltaic resources, diesel generator, electrical energy storage, and combined heat and power generations to serve electrical and thermal loads at the lower-level of energy management system (EMS). All distributed energy resources (DERs) and customers are modelled as self-interested agents who adopt reinforcement learning to optimize their behaviours and operation costs. Based on this algorithm, agents have the capability to interact with each other in a distributed manner and find the best strategy in competitive environment. At the upper-level of EMS, there is an energy management agent that gathers the information of agents of lower-level and clears the MG electrical and thermal energy market in line with predetermined goals. Utilizing energy availability from different DERs and variety of customers\textquoteright{} consumption patterns, considering uncertainty of renewable generation and load consumption and taking into account technical constraint of DERs are the strengths of the presented framework. Performance of the proposed algorithm is investigated under different conditions of agents learning and using e -greedy, soft-max and upper confidence bound methods. The simulation results verify efficacy of the proposed approach.},
}


@misc{LMulti,
	author = {{L. Bollinger} and {R. Evins}},
	title = {Multi-agent reinforcement learning for optimizing technology deployment in distributed multi-energy systems},
	abstract = {Optimal use of distributed renewable sources of energy requires effective and efficient techniques for optimizing technology investment and dispatch in distributed multi-energy systems (DMES). State-of-the-art approaches for optimizing DMES are often formulated as mixed-integer linear programmes, which are limited in terms of their scalability. This paper describes an initial attempt to address DMES optimization using an alternative approach based on multi-agent reinforcement learning. It describes the implementation and results of a multi-agent model implementing two different learning techniques, Q-learning and CACLA. The results indicate that both techniques are able to achieve near-optimal solutions for multi-energy generation dispatch in a DMES without storage. With storage implemented, neither of the multi-agent models reaches a solution close in optimality to that of the equivalent MILP model. Despite this shortcoming, it is concluded that multiagent reinforcement learning shows promise in addressing specific classes of energy hub problems, especially those dealing with large scales.},
}


@inproceedings{Leo2014Multi,
	author = {Leo, R and Milton, R S and Kaviya, A},
	booktitle = {2014 {IEEE} {International} {Conference} on {Computational} {Intelligence} and {Computing} {Research}},
	year = {2014},
	month = {12},
	organization = {IEEE},
	title = {Multi agent reinforcement learning based distributed optimization of solar microgrid},
	url = {http://dx.doi.org/10.1109/ICCIC.2014.7238438},
	doi = {10.1109/iccic.2014.7238438},
	abstract = {We consider grid connected solar microgrid system which contains a local consumers, solar photo voltaic (PV) systems, load and battery. The consumer as an agent continuously interacts with the environment and learns to take optimal actions through a model-free Reinforcement Learning algorithm, namely Q Learning. The aim of the agent is to optimally schedule the battery to increase the utility of the battery and solar photo voltaic system and thereby aims for the long term objective of reducing the power consumption from grid. Multiple agents sense the states of environment components and make collective decisions about how to respond to randomness in load and intermittent solar power by using a Multi agent reinforcement algorithm, namely Coordinated Q Learning (CQ Learning). Each agent learns to optimize individually and contribute to global optimization. Grid power consumed when solar PV system operates individually, by using Q learning is compared with operation of many such solar PV systems in a distributed environment using CQ learning and it is proved that the grid power requirement is considerably reduced in CQ learning than in Q learning. Simulation results using real numerical data are presented for a reliability test of the system.},
}


@inproceedings{Liul2022Autonomous,
	author = {Liul, Jian and Xul, Weifeng and Liul, Zhijun and Ful, Guanhua and Jiang, Yunpeng and Wang, Jun},
	booktitle = {2022 {IEEE} {PES} {Innovative} {Smart} {Grid} {Technologies} - {Asia} ({ISGT} {Asia})},
	year = {2022},
	month = {nov 1},
	organization = {IEEE},
	title = {Autonomous {Decentralized} {Control} of {Distributed} {Generation} using {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://dx.doi.org/10.1109/ISGTAsia54193.2022.10003595},
	doi = {10.1109/isgtasia54193.2022.10003595},
	abstract = {With the large-scale integration of distributed generations (DGs), a centralized control approach is being challenged concerning communication efficiency, resiliency to communication failure, privacy, and scalability. An autonomous decentralized control for DGs in distribution networks is proposed to ensure privacy while also reducing the computational burden. Meanwhile,a multi-agent reinforcement learning method considering privacy protection constraints is developed to solve the autonomous decentralized control problem. The effectiveness of the proposed control method is verified by the numerical example of the modified 141-bus distribution system test\textunderscore{}feeder.},
}


@article{Guo2023Multi,
	author = {Guo, Guodong and Gong, Yanfeng},
	journal = {Applied Sciences},
	number = {5},
	year = {2023},
	month = {feb 23},
	pages = {2865},
	publisher = {MDPI AG},
	title = {Multi-{Microgrid} {Energy} {Management} {Strategy} {Based} on {Multi}-{Agent} {Deep} {Reinforcement} {Learning} with {Prioritized} {Experience} {Replay}},
	volume = {13},
	url = {http://dx.doi.org/10.3390/app13052865},
	doi = {10.3390/app13052865},
	abstract = {The multi-microgrid (MMG) system has attracted more and more attention due to its low carbon emissions and flexibility. This paper proposes a multi-agent reinforcement learning algorithm for real-time energy management of an MMG. In this problem, the MMG is connected to a distribution network (DN). The distribution network operator (DSO) and each microgrid (MG) are modeled as autonomous agents. Each agent makes decisions to suit its interests based on local information. The decision-making problem of multiple agents is modeled as a Markov game and solved by the prioritized multi-agent deep deterministic policy gradient (PMADDPG), where only local observation is required for each agent to make decisions, the centralized training mechanism is applied to learn coordination strategy, and a prioritized experience replay (PER) strategy is adopted to improve learning efficiency. The proposed method can deal with the non-stationary problems in the process of a multi-agent game with partial observable information. In the execution stage, all trained agents are deployed in a distributed manner and make decisions in real time. Simulation results show that according to the proposed method, the training process of a multi-agent game is accelerated, and multiple agents can make optimal decisions only by local information.},
}


@article{Shojaeighadikolaei2022Distributed,
	author = {Shojaeighadikolaei, Amin and Ghasemi, Arman and Jones, Kailani and Dafalla, Yousif and Bardas, Alexandru G. and Ahmadi, Reza and Haashemi, Morteza},
	year = {2022},
	publisher = {arXiv},
	title = {Distributed {Energy} {Management} and {Demand} {Response} in {Smart} {Grids}: A {Multi}-{Agent} {Deep} {Reinforcement} {Learning} {Framework}},
	url = {https://arxiv.org/abs/2211.15858},
	doi = {10.48550/ARXIV.2211.15858},
	abstract = {This paper presents a multi-agent Deep Reinforcement Learning (DRL) framework for autonomous control and integration of renewable energy resources into smart power grid systems. In particular, the proposed framework jointly considers demand response (DR) and distributed energy management (DEM) for residential end-users. DR has a widely recognized potential for improving power grid stability and reliability, while at the same time reducing end-users\textquoteright{} energy bills. However, the conventional DR techniques come with several shortcomings, such as the inability to handle operational uncertainties while incurring end-user disutility, which prevents widespread adoption in real-world applications. The proposed framework addresses these shortcomings by implementing DR and DEM based on real-time pricing strategy that is achieved using deep reinforcement learning. Furthermore, this framework enables the power grid service provider to leverage distributed energy resources (i.e., PV rooftop panels and battery storage) as dispatchable assets to support the smart grid during peak hours, thus achieving management of distributed energy resources. Simulation results based on the Deep Q-Network (DQN) demonstrate significant improvements of the 24-hour accumulative profit for both prosumers and the power grid service provider, as well as major reductions in the utilization of the power grid reserve generators.},
}


@article{Hu2022Multi,
	author = {Hu, Daner and Ye, Zhenhui and Gao, Yuanqi and Ye, Zuzhao and Peng, Yonggang and Yu, Nanpeng},
	journal = {IEEE Transactions on Smart Grid},
	number = {6},
	year = {2022},
	month = {11},
	pages = {4873--4886},
	publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
	title = {Multi-{Agent} {Deep} {Reinforcement} {Learning} for {Voltage} {Control} {With} {Coordinated} {Active} and {Reactive} {Power} {Optimization}},
	volume = {13},
	url = {http://dx.doi.org/10.1109/TSG.2022.3185975},
	doi = {10.1109/tsg.2022.3185975},
	abstract = {The increasing penetration of distributed renewable energy resources causes voltage fluctuations in distribution networks. The controllable active and reactive power resources such as energy storage (ES) systems and electric vehicles (EVs) in active distribution networks play an important role in mitigating the voltage excursions. This paper proposes a two-timescale hybrid voltage control strategy based on a mixed-integer optimization method and multi-agent reinforcement learning (MARL) to reduce power loss and mitigate voltage violations. In the slow-timescale, the active and reactive power optimization problem involving capacitor banks (CBs), on-load tap changers (OLTC), and ES systems is formulated as a mixed-integer second-order cone programming problem. In the fast-timescale, the reactive power of smart inverters connected to solar photovoltaic systems and active power of EVs are adjusted to mitigate short-term voltage fluctuations with a MARL algorithm. Specifically, we propose an experience augmented multi-agent actor-critic (EA-MAAC) algorithm with an attention mechanism to learn high-quality control policies. The control policies are executed online in a decentralized manner. The proposed hybrid voltage control strategy is validated on an IEEE testing distribution feeder. The numerical results show that our proposed control strategy is not only sample-efficient and robust but also effective in mitigating voltage fluctuations.},
}


@article{Sharma2019Distributed,
	author = {Sharma, Mohit K. and Zappone, Alessio and Assaad, Mohamad and Debbah, Merouane and Vassilaras, Spyridon},
	journal = {IEEE Transactions on Cognitive Communications and Networking},
	number = {4},
	year = {2019},
	month = {12},
	pages = {1140--1154},
	publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
	title = {Distributed {Power} {Control} for {Large} {Energy} {Harvesting} {Networks}: A {Multi}-{Agent} {Deep} {Reinforcement} {Learning} {Approach}},
	volume = {5},
	url = {http://dx.doi.org/10.1109/TCCN.2019.2949589},
	doi = {10.1109/tccn.2019.2949589},
	abstract = {In this paper, we develop a multi-agent reinforcement learning (MARL) framework to obtain online power control policies for a large energy harvesting (EH) multiple access channel, when only causal information about the EH process and wireless channel is available. In the proposed framework, we model the online power control problem as a discrete-time mean-field game (MFG), and analytically show that the MFG has a unique stationary solution. Next, we leverage the fictitious play property of the mean-field games, and the deep reinforcement learning technique to learn the stationary solution of the game, in a completely distributed fashion. We analytically show that the proposed procedure converges to the unique stationary solution of the MFG. This, in turn, ensures that the optimal policies can be learned in a completely distributed fashion. In order to benchmark the performance of the distributed policies, we also develop a deep neural network (DNN) based centralized as well as distributed online power control schemes. Our simulation results show the efficacy of the proposed power control policies. In particular, the DNN based centralized power control policies provide a very good performance for large EH networks for which the design of optimal policies is intractable using the conventional methods such as Markov decision processes. Further, performance of both the distributed policies is close to the throughput achieved by the centralized policies.},
}

